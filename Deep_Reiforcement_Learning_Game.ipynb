{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Reiforcement Learning - Game.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN3YSRptyqRyc/v6IswWddb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcosrabaioli/django-rest/blob/master/Deep_Reiforcement_Learning_Game.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsZQl2hDqJBv"
      },
      "source": [
        "Esse notebook é baseado no tutorial disponivel em:\r\n",
        "- https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af\r\n",
        "\r\n",
        "- https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq6N9Ckrp83Y"
      },
      "source": [
        "import gym\r\n",
        "import gym.spaces\r\n",
        "\r\n",
        "# nome do jogo\r\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \r\n",
        "test_env = gym.make(DEFAULT_ENV_NAME)\r\n",
        "# espaço de acoes possiveis\r\n",
        "print(test_env.action_space.n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcLXNJcArO57"
      },
      "source": [
        "# Descricao das acoes do jogo\r\n",
        "# NOOP = FIRE, RIGTH = RIGTHFIRE, LEFT = LEFTFIRE\r\n",
        "#   PARADO        DIREITA            ESQUERDA\r\n",
        "print(test_env.unwrapped.get_action_meanings())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmD9eMpQrmKZ"
      },
      "source": [
        "# Espaço dimenssional do ambiente (neste caso é o tamanho da imagem do atari)\r\n",
        "print(test_env.observation_space.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB_xESAou8Qo"
      },
      "source": [
        "# definindo o hadware\r\n",
        "!nvidia-smi "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ibA_G2uiZE"
      },
      "source": [
        "Criando algumas funçoes para iniciar o jogo, reiniciar e tomar açoes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZC-s8SfsDFA"
      },
      "source": [
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import collections\r\n",
        "\r\n",
        "class FireResetEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env=None):\r\n",
        "        super(FireResetEnv, self).__init__(env)\r\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\r\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        return self.env.step(action)\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.env.reset()\r\n",
        "        obs, _, done, _ = self.env.step(1)\r\n",
        "        if done:\r\n",
        "            self.env.reset()\r\n",
        "        obs, _, done, _ = self.env.step(2)\r\n",
        "        if done:\r\n",
        "            self.env.reset()\r\n",
        "        return obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOqbzL3ixioZ"
      },
      "source": [
        "Vamos criar uma classe para diminuir a amostragem da tela, por questoes de perfrmance é melhor dividirmos por 4 a amostragem, pois é mais simples e efetivo tomar uma decição a cada 4 quadros. Para uma rede neural seria muito custoso tomar uma decisão a cada quadro e pouco acrescentaria na escolha da ação pois a mudança de um quadro para o outro é minima. Existe tbm uma variabilidade na posição dos pixels de um quadro ímpar para um quadro par, entao amostramos os ultimos 2 pixels e extraimos o max deles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G04Q_7Skv2vP"
      },
      "source": [
        "class MaxAndSkipEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env=None, skip=4):\r\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\r\n",
        "        # most recent raw observations (for max pooling across time steps)\r\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\r\n",
        "        self._skip = skip\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        total_reward = 0.0\r\n",
        "        done = None\r\n",
        "        for _ in range(self._skip):\r\n",
        "            obs, reward, done, info = self.env.step(action)\r\n",
        "            self._obs_buffer.append(obs)\r\n",
        "            total_reward += reward\r\n",
        "            if done:\r\n",
        "                break\r\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\r\n",
        "        return max_frame, total_reward, done, info\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self._obs_buffer.clear()\r\n",
        "        obs = self.env.reset()\r\n",
        "        self._obs_buffer.append(obs)\r\n",
        "        return obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y4NcYAizxXG"
      },
      "source": [
        "Vamos agora criar uma classe que reduz o tamanho da imagem de 210x160 rgb para uma escala de 84x84 em escala de cinza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_xtanH9zxyF"
      },
      "source": [
        "class ProcessFrame84(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env=None):\r\n",
        "        super(ProcessFrame84, self).__init__(env)\r\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\r\n",
        "\r\n",
        "    def observation(self, obs):\r\n",
        "        return ProcessFrame84.process(obs)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def process(frame):\r\n",
        "        if frame.size == 210 * 160 * 3:\r\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\r\n",
        "        elif frame.size == 250 * 160 * 3:\r\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\r\n",
        "        else:\r\n",
        "            assert False, \"Unknown resolution.\"\r\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\r\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\r\n",
        "        x_t = resized_screen[18:102, :]\r\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\r\n",
        "        return x_t.astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op5XpInB1B9W"
      },
      "source": [
        "Precisamos obedecer a propeidade de markov para poder implementar um agente. Como precisamos saber a direção da bolinha, e com somente um frame não conseguimos saber, vamos criar um estado que contenha os últimos 4 frames (estados) (ja passado pelo processo de reamostragem da classe MaxAndSkipEnv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPjRhy4M1CGm"
      },
      "source": [
        "class BufferWrapper(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\r\n",
        "        super(BufferWrapper, self).__init__(env)\r\n",
        "        self.dtype = dtype\r\n",
        "        old_space = env.observation_space\r\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\r\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\r\n",
        "        return self.observation(self.env.reset())\r\n",
        "\r\n",
        "    def observation(self, observation):\r\n",
        "        self.buffer[:-1] = self.buffer[1:]\r\n",
        "        self.buffer[-1] = observation\r\n",
        "        return self.buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8_JopAW2ahq"
      },
      "source": [
        "Vamos criar uma classe para mudar o formato de entrada para o formato do pytorch de\r\n",
        "HWC (height, width, channel) para CHW (channel, height, width) format required by PyTorch. Vamos tbm criar uma classe para normalizar a entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U_39JB42tPS"
      },
      "source": [
        "class ImageToPyTorch(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        super(ImageToPyTorch, self).__init__(env)\r\n",
        "        old_shape = self.observation_space.shape\r\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \r\n",
        "                                old_shape[0], old_shape[1]), dtype=np.float32)\r\n",
        "\r\n",
        "    def observation(self, observation):\r\n",
        "        return np.moveaxis(observation, 2, 0)\r\n",
        "\r\n",
        "\r\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\r\n",
        "    def observation(self, obs):\r\n",
        "        return np.array(obs).astype(np.float32) / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRpv299e3EdE"
      },
      "source": [
        "Agora vamos criar uma função que inicializa o ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al-jLRb53JgC"
      },
      "source": [
        "def make_env(env_name):\r\n",
        "    env = gym.make(env_name)\r\n",
        "    env = MaxAndSkipEnv(env)\r\n",
        "    env = FireResetEnv(env)\r\n",
        "    env = ProcessFrame84(env)\r\n",
        "    env = ImageToPyTorch(env)\r\n",
        "    env = BufferWrapper(env, 4)\r\n",
        "    return ScaledFloatFrame(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lcAg9OI403l"
      },
      "source": [
        "Agora vamos ao modelo da rede de Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wUDNt9141Kk"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn        # Pytorch neural network package\r\n",
        "import torch.optim as optim  # Pytorch optimization package\r\n",
        "\r\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNqihhSl5WAt"
      },
      "source": [
        "Primeira camada convolucional:\r\n",
        "\r\n",
        "- As imagens da tela são primeiro processadas por três camadas convolucionais. Isso permite que o sistema explore as relações espaciais e pode explorar o espaço de regra espacial. Além disso, como quatro quadros são empilhados e fornecidos como entrada, essas camadas convolucionais também extraem algumas propriedades temporais entre esses quadros\r\n",
        "- As camadas convolucionais são seguidas por uma camada oculta totalmente conectada com ativação ReLU e uma camada de saída linear totalmente conectada que produziu o vetor de valores de ação:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhHP8d6G49Rd"
      },
      "source": [
        "# Taken from \r\n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class DQN(nn.Module):\r\n",
        "    def __init__(self, input_shape, n_actions):\r\n",
        "        super(DQN, self).__init__()\r\n",
        "\r\n",
        "        # Primeira camada convolucional\r\n",
        "        self.conv = nn.Sequential(\r\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "        # Segunda camada full connected\r\n",
        "        conv_out_size = self._get_conv_out(input_shape)\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            nn.Linear(conv_out_size, 512),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(512, n_actions)\r\n",
        "        )\r\n",
        "\r\n",
        "    def _get_conv_out(self, shape):\r\n",
        "        o = self.conv(torch.zeros(1, *shape))\r\n",
        "        return int(np.prod(o.size()))\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\r\n",
        "        return self.fc(conv_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73RWtZdl7fpR"
      },
      "source": [
        "test_env = make_env(DEFAULT_ENV_NAME)\r\n",
        "test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\r\n",
        "print(test_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up8_MT_iOHiV"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5byqAcJ8Tgld"
      },
      "source": [
        "- MEAN_REWARD_BOUND: o limite de recompensa para parar de treinar. Vamos considerar que o jogo convergiu quando o nosso agente atingir uma média de 19 jogos ganhos (de 21) nos últimos 100 jogos\r\n",
        "- gamma: o fator de desconto\r\n",
        "- batch_size: o tamanho do minibatch no experience replay\r\n",
        "- learning_rate: é a taxa de aprendizagem\r\n",
        "- replay_size: tamanho do buffer de reprodução (número máximo de experiências armazenadas na memória de reprodução)\r\n",
        "- sync_target_frames: indica a frequência com que sincronizamos pesos de modelo da rede DQN principal para a rede DQN target network (quantos quadros entre a sincronização)\r\n",
        "- replay_start_size: a contagem de quadros (experiências) para adicionar ao buffer de reprodução antes de iniciar o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdkLRrWFOIw1"
      },
      "source": [
        "import time\r\n",
        "import numpy as np\r\n",
        "import collections\r\n",
        "\r\n",
        "# Limite de recompensa para parar o treinamento\r\n",
        "MEAN_REWARD_BOUND = 19.0           \r\n",
        "\r\n",
        "gamma = 0.99                   \r\n",
        "batch_size = 32                \r\n",
        "replay_size = 10000            \r\n",
        "learning_rate = 1e-4           \r\n",
        "sync_target_frames = 1000      \r\n",
        "replay_start_size = 10000      \r\n",
        "\r\n",
        "eps_start=1.0\r\n",
        "eps_decay=.999985\r\n",
        "eps_min=0.02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeuTpX6DOUqf"
      },
      "source": [
        "**Experience Replay**: a ideia básica por trás da repetição da experiência é armazenar experiências passadas e, em seguida, usar um subconjunto aleatório dessas experiências para atualizar a Q-network, em vez de usar apenas a experiência mais recente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRtFdZrNONFM"
      },
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\r\n",
        "\r\n",
        "class ExperienceReplay:\r\n",
        "    def __init__(self, capacity):\r\n",
        "        self.buffer = collections.deque(maxlen=capacity)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.buffer)\r\n",
        "\r\n",
        "    def append(self, experience):\r\n",
        "        self.buffer.append(experience)\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\r\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\r\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\r\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uqeP5M0PEyf"
      },
      "source": [
        "class Agent:\r\n",
        "    def __init__(self, env, exp_buffer):\r\n",
        "        self.env = env\r\n",
        "        self.exp_buffer = exp_buffer\r\n",
        "        self._reset()\r\n",
        "\r\n",
        "    def _reset(self):\r\n",
        "        self.state = env.reset()\r\n",
        "        self.total_reward = 0.0\r\n",
        "\r\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\r\n",
        "\r\n",
        "        done_reward = None\r\n",
        "        # escolhe uma ação aleatoria com um probabilidade epsilon\r\n",
        "        if np.random.random() < epsilon:\r\n",
        "            action = env.action_space.sample()\r\n",
        "        # escolhe uma ação definida pela politica com probabilidade 1-epsilon\r\n",
        "        else:\r\n",
        "            state_a = np.array([self.state], copy=False)\r\n",
        "            state_v = torch.tensor(state_a).to(device)\r\n",
        "            q_vals_v = net(state_v)\r\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\r\n",
        "            action = int(act_v.item())\r\n",
        "        # Verifica o próximo estado e a recompensa\r\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\r\n",
        "        self.total_reward += reward\r\n",
        "\r\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\r\n",
        "        self.exp_buffer.append(exp)\r\n",
        "        self.state = new_state\r\n",
        "        if is_done:\r\n",
        "            done_reward = self.total_reward\r\n",
        "            self._reset()\r\n",
        "        return done_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6guFm_7LjPlG"
      },
      "source": [
        "import datetime\r\n",
        "print(\">>>Training starts at \",datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIyo2ocimjkU"
      },
      "source": [
        "env = make_env(DEFAULT_ENV_NAME)\r\n",
        "\r\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\r\n",
        "target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\r\n",
        "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\r\n",
        " \r\n",
        "buffer = ExperienceReplay(replay_size)\r\n",
        "agent = Agent(env, buffer)\r\n",
        "\r\n",
        "epsilon = eps_start\r\n",
        "\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\r\n",
        "total_rewards = []\r\n",
        "frame_idx = 0  \r\n",
        "\r\n",
        "best_mean_reward = None\r\n",
        "\r\n",
        "while True:\r\n",
        "        frame_idx += 1\r\n",
        "        # Atualizando a probabilidade de escolha de ação aleatória\r\n",
        "        epsilon = max(epsilon*eps_decay, eps_min)\r\n",
        "\r\n",
        "        # Usando a rede para selecionar e fazer a ação\r\n",
        "        reward = agent.play_step(net, epsilon, device=device)\r\n",
        "        # Reward é somente diferente de nulo quando um jogo acaba\r\n",
        "        if reward is not None:\r\n",
        "            total_rewards.append(reward)\r\n",
        "            \r\n",
        "            # Caculando a media de pontos dos ultimos 100 jogos\r\n",
        "            mean_reward = np.mean(total_rewards[-100:])\r\n",
        "\r\n",
        "            print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (\r\n",
        "                frame_idx, len(total_rewards), mean_reward, epsilon))\r\n",
        "            \r\n",
        "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\r\n",
        "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\r\n",
        "            writer.add_scalar(\"reward\", reward, frame_idx)\r\n",
        "\r\n",
        "            # Atualizando o melher score e salvando o modelo de melhor score\r\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\r\n",
        "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\r\n",
        "                best_mean_reward = mean_reward\r\n",
        "                if best_mean_reward is not None:\r\n",
        "                    print(\"Best mean reward updated %.3f\" % (best_mean_reward))\r\n",
        "\r\n",
        "            # Critério de parada do aprendizado\r\n",
        "            if mean_reward > MEAN_REWARD_BOUND:\r\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\r\n",
        "                break\r\n",
        "\r\n",
        "        # Verifica se ja tem experiencia suficiente para usar no experience replay\r\n",
        "        # O processo de aprendizado somente começa depois de o agente ter explorado\r\n",
        "        # o ambiente por algumas vezes, definido por replay_start_size\r\n",
        "        if len(buffer) < replay_start_size:\r\n",
        "            continue\r\n",
        "        # Buscando na memoria algumas experiencias passadas\r\n",
        "        batch = buffer.sample(batch_size)\r\n",
        "        states, actions, rewards, dones, next_states = batch\r\n",
        "\r\n",
        "        # Formatação de dados para pytorch\r\n",
        "        states_v = torch.tensor(states).to(device)\r\n",
        "        next_states_v = torch.tensor(next_states).to(device)\r\n",
        "        actions_v = torch.tensor(actions).to(device)\r\n",
        "        rewards_v = torch.tensor(rewards).to(device)\r\n",
        "        done_mask = torch.ByteTensor(dones).to(device)\r\n",
        "\r\n",
        "        # Obtendo a predição pela funçao Q para cada estado e ação dada (state-action values)\r\n",
        "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\r\n",
        "\r\n",
        "        # Predizendo o valor da recompensa da melhor ação para o próximo estado \r\n",
        "        next_state_values = target_net(next_states_v).max(1)[0]\r\n",
        "\r\n",
        "        # Se o estado for o último então nao ha recompensa\r\n",
        "        next_state_values[done_mask] = 0.0\r\n",
        "\r\n",
        "        # fazendo uma cópia da variavel para evitar problemas de concorrencia de dados\r\n",
        "        next_state_values = next_state_values.detach()\r\n",
        "        \r\n",
        "        # calculando o valor da recompensa esperado (predicao da prox recompensa + recompensa dada pela experiencia)\r\n",
        "        expected_state_action_values = next_state_values * gamma + rewards_v\r\n",
        "\r\n",
        "        \r\n",
        "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss_t.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Atualiza a target network após cada passo de sincronização\r\n",
        "        if frame_idx % sync_target_frames == 0:\r\n",
        "            target_net.load_state_dict(net.state_dict())\r\n",
        "       \r\n",
        "writer.close()\r\n",
        "\r\n",
        "        \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzjvk8Fm4i83"
      },
      "source": [
        "print(\">>>Training ends at \",datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm087Ijg4pT3"
      },
      "source": [
        "tensorboard  --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR45q8fv4tkH"
      },
      "source": [
        "import gym\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import torch\r\n",
        "\r\n",
        "import collections\r\n",
        "\r\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\r\n",
        "FPS = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-IoKHLQ46L4"
      },
      "source": [
        "Alguns passos para pode exibir o resultado em um jogo aqui no colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edQQuzS34yGv"
      },
      "source": [
        "# Taken from \r\n",
        "# https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\r\n",
        "\r\n",
        "!apt-get install -y xvfb x11-utils\r\n",
        "\r\n",
        "!pip install pyvirtualdisplay==0.2.* \\\r\n",
        "             PyOpenGL==3.1.* \\\r\n",
        "             PyOpenGL-accelerate==3.1.*\r\n",
        "\r\n",
        "!pip install gym[box2d]==0.17.*\r\n",
        "\r\n",
        "import pyvirtualdisplay\r\n",
        "\r\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\r\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIV76k2f5KZX"
      },
      "source": [
        "Preparamos o código que gera um vídeo com um episódio do jogo. O vídeo é armazenado na pasta video.\r\n",
        "\r\n",
        "O código é quase uma cópia exata do método play_step () da classe Agent, sem a seleção de ação epsilon-greedy. Basta passarmos nossa observação para o agente e selecionarmos a ação com valor máximo. A única coisa nova aqui é o método render () no Environment, que é uma forma padrão no Gym de exibir a observação atual.\r\n",
        "\r\n",
        "O loop principal no código passa a ação para o ambiente, conta a recompensa total e interrompe o loop quando o episódio termina. Após o episódio, mostra a recompensa total e a contagem de vezes que o agente executou a ação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLlWUnRI5Asn"
      },
      "source": [
        "# Taken (partially) from \r\n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/03_dqn_play.py\r\n",
        "\r\n",
        "\r\n",
        "model='PongNoFrameskip-v4-best.dat'\r\n",
        "record_folder=\"video\"  \r\n",
        "visualize=True\r\n",
        "\r\n",
        "env = make_env(DEFAULT_ENV_NAME)\r\n",
        "if record_folder:\r\n",
        "        env = gym.wrappers.Monitor(env, record_folder, force=True)\r\n",
        "net = DQN(env.observation_space.shape, env.action_space.n)\r\n",
        "net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\r\n",
        "\r\n",
        "state = env.reset()\r\n",
        "total_reward = 0.0\r\n",
        "\r\n",
        "while True:\r\n",
        "        start_ts = time.time()\r\n",
        "        if visualize:\r\n",
        "            env.render()\r\n",
        "        state_v = torch.tensor(np.array([state], copy=False))\r\n",
        "        q_vals = net(state_v).data.numpy()[0]\r\n",
        "        action = np.argmax(q_vals)\r\n",
        "        \r\n",
        "        state, reward, done, _ = env.step(action)\r\n",
        "        total_reward += reward\r\n",
        "        if done:\r\n",
        "            break\r\n",
        "        if visualize:\r\n",
        "            delta = 1/FPS - (time.time() - start_ts)\r\n",
        "            if delta > 0:\r\n",
        "                time.sleep(delta)\r\n",
        "print(\"Total reward: %.2f\" % total_reward)\r\n",
        "\r\n",
        "if record_folder:\r\n",
        "        env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}